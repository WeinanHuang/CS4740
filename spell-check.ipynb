{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import collections\n",
    "import nltk\n",
    "import random\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dic_loc = 'C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\\\confusion_set.txt'\n",
    "head = 'C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\\\'\n",
    "text_type = ['atheism', 'autos', 'graphics','medicine','motorcycles','religion','space']\n",
    "tail = '\\\\train_docs\\*.txt'\n",
    "test_head = 'C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\\\'\n",
    "test_tail = '\\\\test_modified_docs\\*.txt'\n",
    "test_head = 'C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\\\'\n",
    "test_tail = '\\\\test_modified_docs\\*.txt'\n",
    "output_head = 'C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\\\'\n",
    "output_tail = '\\\\train_genr_docs\\\\'\n",
    "k = 2 # threshold for ukw\n",
    "\n",
    "# read pairs of words that we need to distinguish\n",
    "def read_target_word(dic_loc):\n",
    "    data1 = []\n",
    "    data2 = []\n",
    "\n",
    "    f = open(dic_loc, 'r')\n",
    "    for line in f:\n",
    "        temp = line.split()\n",
    "        data1.append(temp[0])\n",
    "        data2.append(temp[1])\n",
    "    # # needed?\n",
    "    data1[0] = 'went'\n",
    "\n",
    "    return data1, data2, data1 + data2\n",
    "\n",
    "def txt_clean(filepath):\n",
    "    #first set up some string to cut off the head of the e-mail\n",
    "    # headStr1 = 'writes :'\n",
    "    # headStr2 = 'wrote :'\n",
    "    # headStr3 = 'said :'\n",
    "    # headStr4 = 'Subject : Re : '\n",
    "    # headStr5 = 'Subject : '\n",
    "\n",
    "    #this regular expression is set to capture the email address\n",
    "    regex = re.compile((\"([a-z0-9!#$%&'*+\\/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+\\/=?^_`\"\n",
    "                        \"{|}~-]+)*(@|\\sat\\s)(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?(\\.|\"\n",
    "                        \"\\sdot\\s))+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?)\"))\n",
    "\n",
    "    #this is to set a regular expression which will be used to capture the first occurance of letter\n",
    "    r_head = re.compile(\"([a-zA-Z]+?)\")\n",
    "\n",
    "    # set up Text\n",
    "    Text = ''\n",
    "\n",
    "    #read all the file now\n",
    "    files = glob.glob(filepath)\n",
    "    for file in files:\n",
    "\n",
    "        Text_Aggregate = ''\n",
    "        f = open(file, 'r')\n",
    "        line = f.read().replace('\\n', '').lower()\n",
    "\n",
    "        # # leave out head (Subject, Email Address, etc)\n",
    "        # if line.rfind(headStr1) != -1:\n",
    "        #     ind = line.rfind(headStr1)\n",
    "        #     data = line[(ind+len(headStr1)):]\n",
    "        # elif line.rfind(headStr2) != -1:\n",
    "        #     ind = line.rfind(headStr2)\n",
    "        #     data = line[(ind+len(headStr2)):]\n",
    "        # elif line.rfind(headStr3) != -1:\n",
    "        #     ind = line.rfind(headStr3)\n",
    "        #     data = line[(ind + len(headStr3)):]\n",
    "        # elif line.rfind(headStr4) != -1:\n",
    "        #     ind = line.rfind(headStr4)\n",
    "        #     data = line[(ind + len(headStr4)):]\n",
    "        # elif line.rfind(headStr5) != -1:\n",
    "        #     ind = line.rfind(headStr5)\n",
    "        #     data = line[(ind + len(headStr5)):]\n",
    "        # else:\n",
    "        #     data = line\n",
    "        data = line\n",
    "\n",
    "        #clean the symbol >\n",
    "        data_clear_symbol = re.sub('[>]', '', data)\n",
    "\n",
    "        #this is to clean the signture after - - -\n",
    "        idx = data_clear_symbol.find('- - -')\n",
    "        if (data_clear_symbol.find('- - -') != -1 and len(data_clear_symbol[idx+1:]) <150):\n",
    "            data_c1 = data_clear_symbol[0:idx+1]\n",
    "        else:\n",
    "            data_c1 = data_clear_symbol\n",
    "\n",
    "        #this is to clean the signture after - -\n",
    "        idx2 = data_c1.find('- -')\n",
    "        if (data_c1.find('- -') != -1 and len(data_c1[idx2+1:]) <150):\n",
    "            data_clear_sig = data_c1[0:idx2+1]\n",
    "        else:\n",
    "            data_clear_sig = data_c1\n",
    "\n",
    "        #delete all the email address\n",
    "        for email in re.findall(regex, data_clear_sig):\n",
    "            data_clear_sig = data_clear_sig.replace(email[0],'')\n",
    "\n",
    "        #replace all the \" ' \" to space\n",
    "        data_clear_sig = data_clear_sig.replace(\" ' \",'')\n",
    "\n",
    "        # replace uneccesary notation\n",
    "        rmList = '> \" | # : - ) ( * [ ] } { + = ^ __ ~ / \\\\'\n",
    "        rmList = rmList.split()\n",
    "        for n in rmList:\n",
    "            data_clear_sig = data_clear_sig.replace(n, '')\n",
    "\n",
    "        # switch multiple blanks into single ones\n",
    "        data_postclean = ' '.join(data_clear_sig.split())\n",
    "\n",
    "        #let all the string tart from letter and end with letter\n",
    "        idx_head = re.search(r_head, data_postclean)\n",
    "        idx_gethead = idx_head.start()\n",
    "        data_after = data_postclean[idx_gethead:]\n",
    "\n",
    "        #print file, '\\n', data_after, '\\n'\n",
    "        data_after = data_after.strip()\n",
    "\n",
    "        #replace all the ...\n",
    "        sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        sen_list = ['<s> ' + s for s in sent_detector.tokenize(data_after.strip())]\n",
    "        sen_str = ' '.join(sen_list)\n",
    "\n",
    "        #get a string for all the email in the folder\n",
    "        Text = Text + ' ' + sen_str\n",
    "        Text = Text + ' <s> '\n",
    "        Text_Aggregate = Text_Aggregate + Text\n",
    "\n",
    "    Data = Text_Aggregate.split(' ')\n",
    "    return Data\n",
    "\n",
    "# decide which words should be UNK, replace those in train set\n",
    "def Set_Unknown(TextList, k, target_list):\n",
    "    ukw_list = []\n",
    "\n",
    "    wd_base = list(set(TextList))\n",
    "    wd_count = collections.Counter(TextList)\n",
    "\n",
    "    for wd in wd_base:\n",
    "        if ( wd not in target_list ) & ( wd_count[wd] < k ):\n",
    "            ukw_list.append(wd)\n",
    "            TextList = ['UKW' if x == wd else x for x in TextList]\n",
    "\n",
    "    return ukw_list, TextList\n",
    "\n",
    "# train bigram model\n",
    "def train_bigram(target_list, TextList):\n",
    "\n",
    "    wd_base = list(set(TextList))\n",
    "    wd_count = collections.Counter(TextList)\n",
    "\n",
    "    BiGram = {}\n",
    "    for wd in wd_base:\n",
    "        BiGram[wd] = {}\n",
    "\n",
    "    for i in range(len(TextList) - 1):\n",
    "        BiGram[TextList[i]][TextList[i + 1]] = 0\n",
    "\n",
    "    for i in range(len(TextList) - 1):\n",
    "        wd = TextList[i]\n",
    "        wd1 = TextList[i+1]\n",
    "        BiGram[wd][wd1] = BiGram[wd][wd1] + 1\n",
    "\n",
    "    for Key in BiGram.keys():\n",
    "        for key, value in BiGram[Key].items():\n",
    "            BiGram[Key][key] = 1. * value / wd_count[Key]\n",
    "\n",
    "    # BiGram = {}\n",
    "    # for wd in wd_base:\n",
    "    #     BiGram[wd] = {}\n",
    "    #\n",
    "    # for i in range(len(TextList) - 1):\n",
    "    #     wd = TextList[i]\n",
    "    #     wd1 = TextList[i+1]\n",
    "    #     if wd1 in target_list:\n",
    "    #         if wd1 not in BiGram[wd]:\n",
    "    #             BiGram[wd][wd1] = 0\n",
    "    #         else:\n",
    "    #             BiGram[wd][wd1] = BiGram[wd][wd1] + 1\n",
    "    #\n",
    "    # for Key in BiGram.keys():\n",
    "    #     for key, value in BiGram[Key].items():\n",
    "    #         BiGram[Key][key] = 1. * value / wd_count[Key]\n",
    "\n",
    "    return wd_base, BiGram\n",
    "\n",
    "# check and change the word if necessary\n",
    "#def check_word(ukw_list, TestList, target_list1, target_list2, target_list, Bigram):\n",
    "def check_word(TestList, target_list1, target_list2, target_list, Bigram):\n",
    "    time = 0\n",
    "\n",
    "    for i in range(len(TestList) - 1):\n",
    "        if TestList[i+1] in target_list1:\n",
    "            p1 = 0\n",
    "            p2 = 0\n",
    "            cwd = target_list2[target_list1.index(TestList[i+1])]\n",
    "\n",
    "            if TestList[i+1] in Bigram[TestList[i]].keys():\n",
    "                p1 = Bigram[TestList[i]][TestList[i+1]]\n",
    "\n",
    "            if cwd in Bigram[TestList[i]].keys():\n",
    "                p2 = Bigram[TestList[i]][cwd]\n",
    "\n",
    "            if p1 < p2:\n",
    "                TestList[i+1] = cwd\n",
    "                time = time + 1\n",
    "                # print 'p1 = ', p1\n",
    "                # print 'p2 = ', p2\n",
    "                continue\n",
    "\n",
    "        if TestList[i+1] in target_list2:\n",
    "            p1 = 0\n",
    "            p2 = 0\n",
    "            cwd = target_list1[target_list2.index(TestList[i+1])]\n",
    "\n",
    "            if TestList[i+1] in Bigram[TestList[i]].keys():\n",
    "                p2 = Bigram[TestList[i]][TestList[i+1]]\n",
    "\n",
    "            if cwd in Bigram[TestList[i]].keys():\n",
    "                p1 = Bigram[TestList[i]][cwd]\n",
    "\n",
    "            if p1 > p2:\n",
    "                TestList[i+1] = cwd\n",
    "                time = time + 1\n",
    "                # print 'p1 = ', p1\n",
    "                # print 'p2 = ', p2\n",
    "                continue\n",
    "\n",
    "    return time, TestList\n",
    "\n",
    "# def main():\n",
    "\n",
    "#     target_list = read_target_word(dic_loc);\n",
    "\n",
    "#     for t in text_type:\n",
    "#         # get list of target words\n",
    "#         target_list1, target_list2, target_list = read_target_word(dic_loc)\n",
    "\n",
    "#         # clean train texts\n",
    "#         filepath = head + t + tail\n",
    "#         data = txt_clean(filepath)\n",
    "\n",
    "#         # get unw vocabulary based on train set\n",
    "#         ukw_list, data = Set_Unknown(data, k, target_list)\n",
    "\n",
    "#         # get vocabulary and train biGram\n",
    "#         wd_base, Bigram = train_bigram(target_list, data)\n",
    "\n",
    "#         # get the the files to be revised in test set\n",
    "#         text_filepath = test_head + t + test_tail\n",
    "#         test_files = glob.glob(text_filepath)\n",
    "\n",
    "#         # for each of the file to be revised\n",
    "#         for test_file in test_files:\n",
    "\n",
    "#             # read file\n",
    "#             f = open(test_file, 'r')\n",
    "#             line = f.read().replace('\\n', '').lower()\n",
    "#             data_test = line.split(' ')\n",
    "\n",
    "#             # assign unknow words\n",
    "#             temp2 = set(wd_base)\n",
    "#             temp2 = list(temp2.difference(set(ukw_list)))\n",
    "#             data_test = ['UKW' if x not in temp2 else x for x in data_test]\n",
    "\n",
    "#             # change the word if necessary\n",
    "#             time, TestList = check_word(data_test, target_list1, target_list2, target_list, Bigram)\n",
    "#             print time\n",
    "\n",
    "#             # output files\n",
    "#             output_file_name = output_head + t + output_tail + test_file[test_file.find(t + '_'):-12] + 'output.txt'\n",
    "\n",
    "#             text_file = open(output_file_name, \"w\")\n",
    "#             text_file.write(' '.join(TestList))\n",
    "#             text_file.close()\n",
    "\n",
    "#             for ii in test_files:\n",
    "#                 print ii\n",
    "\n",
    "\n",
    "#             break\n",
    "#         break\n",
    "\n",
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file0_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file10_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file11_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file12_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file13_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file14_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file15_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file16_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file17_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file18_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file19_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file1_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file20_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file21_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file22_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file23_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file24_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file25_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file26_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file27_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file28_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file29_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file2_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file30_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file31_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file32_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file33_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file34_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file35_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file36_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file37_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file38_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file39_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file3_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file40_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file41_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file42_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file43_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file44_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file45_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file46_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file47_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file48_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file49_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file4_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file5_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file6_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file7_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file8_modified.txt\n",
      "C:\\Users\\Ziyan Liu\\Desktop\\Cornell\\Study\\cs4740\\data\\data_corrected\\spell_checking_task\\atheism\\test_modified_docs\\atheism_file9_modified.txt\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    target_list = read_target_word(dic_loc);\n",
    "\n",
    "    for t in text_type:\n",
    "        # get list of target words\n",
    "        target_list1, target_list2, target_list = read_target_word(dic_loc)\n",
    "\n",
    "        # clean train texts\n",
    "        filepath = head + t + tail\n",
    "        data = txt_clean(filepath)\n",
    "\n",
    "        # get unw vocabulary based on train set\n",
    "        ukw_list, data = Set_Unknown(data, k, target_list)\n",
    "\n",
    "        # get vocabulary and train biGram\n",
    "        wd_base, Bigram = train_bigram(target_list, data)\n",
    "\n",
    "        # get the the files to be revised in test set\n",
    "        text_filepath = test_head + t + test_tail\n",
    "        test_files = glob.glob(text_filepath)\n",
    "\n",
    "        # for each of the file to be revised\n",
    "        for test_file in test_files:\n",
    "\n",
    "            # read file\n",
    "            f = open(test_file, 'r')\n",
    "            line = f.read().replace('\\n', '').lower()\n",
    "            data_test = line.split(' ')\n",
    "\n",
    "            # assign unknow words\n",
    "            temp2 = set(wd_base)\n",
    "            temp2 = list(temp2.difference(set(ukw_list)))\n",
    "            data_test = ['UKW' if x not in temp2 else x for x in data_test]\n",
    "\n",
    "            # change the word if necessary\n",
    "            time, TestList = check_word(data_test, target_list1, target_list2, target_list, Bigram)\n",
    "            print time\n",
    "\n",
    "            # output files\n",
    "            output_file_name = output_head + t + output_tail + test_file[test_file.find(t + '_'):-12] + 'output.txt'\n",
    "\n",
    "            text_file = open(output_file_name, \"w\")\n",
    "            text_file.write(' '.join(TestList))\n",
    "            text_file.close()\n",
    "\n",
    "            for ii in test_files:\n",
    "                print ii\n",
    "\n",
    "\n",
    "            break\n",
    "        break\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
